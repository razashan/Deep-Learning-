{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled12.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOtOMx1_KDta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib.colors import ListedColormap"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFYyzvIdKgRc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tanh(x):\n",
        "  return (1.0 - numpy.exp(-2 * x))/( 1.0  + numpy.exp(-2 * x))\n",
        "\n",
        "def tanh_derivative(x):\n",
        "  return (1+ tanh(x)) * ( 1 - tanh(x))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcA2C9nAK2aB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6390cce3-bfa3-484d-998d-d0d418823349"
      },
      "source": [
        "import numpy\n",
        "class NeuralNetwork:\n",
        "    # net_arch consists of a list of integers, indicating\n",
        "    # the number of neurons in each layer\n",
        "    def __init__(self, net_arch):\n",
        "        self.activation_func = tanh\n",
        "        self.activation_derivative = tanh_derivative\n",
        "        self.layers = len(net_arch)\n",
        "        self.steps_per_epoch = 1000\n",
        "        self.net_arch = net_arch\n",
        "\n",
        "        # initialize the weights with random values in the range (-1,1)\n",
        "        self.weights = []\n",
        "        for layer in range(len(net_arch) - 1):\n",
        "            w = 2 * numpy.random.rand(net_arch[layer] + 1, net_arch[layer + 1]) - 1\n",
        "            self.weights.append(w)\n",
        "\n",
        "    def fit(self, data, labels, learning_rate=0.1, epochs=10):\n",
        "        \"\"\"\n",
        "        :param data: data is the set of all possible pairs of booleans\n",
        "                     True or False indicated by the integers 1 or 0\n",
        "                     labels is the result of the logical operation 'xor'\n",
        "                     on each of those input pairs\n",
        "        :param labels: array of 0/1 for each datum\n",
        "        \"\"\"\n",
        "\n",
        "        # Add bias units to the input layer\n",
        "        ones = numpy.ones((1, data.shape[0]))\n",
        "        Z = numpy.concatenate((ones.T, data), axis=1)\n",
        "        training = epochs * self.steps_per_epoch\n",
        "        for k in range(training):\n",
        "            if k % self.steps_per_epoch == 0:\n",
        "                # print ('epochs:', k/self.steps_per_epoch)\n",
        "                print('epochs: {}'.format(k / self.steps_per_epoch))\n",
        "                for s in data:\n",
        "                    print(s, nn.predict(s))\n",
        "\n",
        "            sample = numpy.random.randint(data.shape[0])\n",
        "            y = [Z[sample]]\n",
        "\n",
        "            for i in range(len(self.weights) - 1):\n",
        "                activation = numpy.dot(y[i], self.weights[i])\n",
        "                activation_f = self.activation_func(activation)\n",
        "                # add the bias for the next layer\n",
        "                activation_f = numpy.concatenate((numpy.ones(1), numpy.array(activation_f)))\n",
        "                y.append(activation_f)\n",
        "\n",
        "            # last layer\n",
        "            activation = numpy.dot(y[-1], self.weights[-1])\n",
        "            activation_f = self.activation_func(activation)\n",
        "            y.append(activation_f)\n",
        "\n",
        "            # error for the output layer\n",
        "            error = labels[sample] - y[-1]\n",
        "            delta_vec = [error * self.activation_derivative(y[-1])]\n",
        "\n",
        "            # we need to begin from the back from the next to last layer\n",
        "            for i in range(self.layers - 2, 0, -1):\n",
        "                error = delta_vec[-1].dot(self.weights[i][1:].T)\n",
        "                error = error * self.activation_derivative(y[i][1:])\n",
        "                delta_vec.append(error)\n",
        "\n",
        "            # reverse\n",
        "            # [level3(output)->level2(hidden)]  => [level2(hidden)->level3(output)]\n",
        "            delta_vec.reverse()\n",
        "\n",
        "            # backpropagation\n",
        "            # 1. Multiply its output delta and input activation \n",
        "            #    to get the gradient of the weight.\n",
        "            # 2. Subtract a ratio (percentage) of the gradient from the weight\n",
        "            for i in range(len(self.weights)):\n",
        "                layer = y[i].reshape(1, nn.net_arch[i] + 1)\n",
        "\n",
        "                delta = delta_vec[i].reshape(1, nn.net_arch[i + 1])\n",
        "                self.weights[i] += learning_rate * layer.T.dot(delta)\n",
        "\n",
        "    def predict(self, x):\n",
        "        val = numpy.concatenate((numpy.ones(1).T, numpy.array(x)))\n",
        "        for i in range(0, len(self.weights)):\n",
        "            val = self.activation_func(numpy.dot(val, self.weights[i]))\n",
        "            val = numpy.concatenate((numpy.ones(1).T, numpy.array(val)))\n",
        "\n",
        "        return val[1]\n",
        "\n",
        "    def plot_decision_regions(self, X, y, points=200):\n",
        "        markers = ('o', '^')\n",
        "        colors = ('red', 'blue')\n",
        "        cmap = ListedColormap(colors)\n",
        "\n",
        "        x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "        x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "\n",
        "        # To produce zoomed-out figures, you can replace the preceding 2 lines with:\n",
        "        # x1_min, x1_max = -10, 11\n",
        "        # x2_min, x2_max = -10, 11\n",
        "\n",
        "        resolution = max(x1_max - x1_min, x2_max - x2_min) / float(points)\n",
        "\n",
        "        xx1, xx2 = numpy.meshgrid(numpy.arange(x1_min,\n",
        "                                               x1_max,\n",
        "                                               resolution),\n",
        "                                  numpy.arange(x2_min, x2_max, resolution))\n",
        "        input = numpy.array([xx1.ravel(), xx2.ravel()]).T\n",
        "        Z = numpy.empty(0)\n",
        "        for i in range(input.shape[0]):\n",
        "            val = nn.predict(numpy.array(input[i]))\n",
        "            if val < 0.5:\n",
        "                val = 0\n",
        "            if val >= 0.5:\n",
        "                val = 1\n",
        "            Z = numpy.append(Z, val)\n",
        "\n",
        "        Z = Z.reshape(xx1.shape)\n",
        "\n",
        "        plt.pcolormesh(xx1, xx2, Z, cmap=cmap)\n",
        "        plt.xlim(xx1.min(), xx1.max())\n",
        "        plt.ylim(xx2.min(), xx2.max())\n",
        "        # plot all samples\n",
        "\n",
        "        classes = [\"False\", \"True\"]\n",
        "\n",
        "        for idx, cl in enumerate(numpy.unique(y)):\n",
        "            plt.scatter(x=X[y == cl, 0],\n",
        "                        y=X[y == cl, 1],\n",
        "                        alpha=1.0,\n",
        "                        c=colors[idx],\n",
        "                        edgecolors='black',\n",
        "                        marker=markers[idx],\n",
        "                        s=80,\n",
        "                        label=classes[idx])\n",
        "\n",
        "        plt.xlabel('x-axis')\n",
        "        plt.ylabel('y-axis')\n",
        "        plt.legend(loc='upper left')\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    numpy.random.seed(0)\n",
        "\n",
        "    # Initialize the NeuralNetwork with 2 input, 2 hidden, and 1 output neurons\n",
        "    nn = NeuralNetwork([2, 2, 1])\n",
        "\n",
        "    X = numpy.array([[0, 0],\n",
        "                     [0, 1],\n",
        "                     [1, 0],\n",
        "                     [1, 1]])\n",
        "\n",
        "    y = numpy.array([0, 1, 1, 0])\n",
        "\n",
        "    nn.fit(X, y, epochs=10)\n",
        "\n",
        "    print(\"Final prediction\")\n",
        "    for s in X:\n",
        "        print(s, nn.predict(s))\n",
        "\n",
        "    nn.plot_decision_regions(X, y)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epochs: 0.0\n",
            "[0 0] 0.31634987228520156\n",
            "[0 1] 0.38455314510086014\n",
            "[1 0] 0.49960366414001517\n",
            "[1 1] 0.5470092417007291\n",
            "epochs: 1.0\n",
            "[0 0] 0.10110562119575021\n",
            "[0 1] 0.4983062530300437\n",
            "[1 0] 0.5483740117095983\n",
            "[1 1] 0.6358128781126651\n",
            "epochs: 2.0\n",
            "[0 0] 0.07164948329787552\n",
            "[0 1] 0.8610758132814952\n",
            "[1 0] 0.8502850626450229\n",
            "[1 1] 0.07158530421971639\n",
            "epochs: 3.0\n",
            "[0 0] 0.01758656789925367\n",
            "[0 1] 0.966663773401989\n",
            "[1 0] 0.9651222166853127\n",
            "[1 1] 0.011468668342141863\n",
            "epochs: 4.0\n",
            "[0 0] -0.0017118993569207345\n",
            "[0 1] 0.9815292663780985\n",
            "[1 0] 0.9828812324283913\n",
            "[1 1] -0.0003037091869647862\n",
            "epochs: 5.0\n",
            "[0 0] 0.0026985374081322524\n",
            "[0 1] 0.9885083594808965\n",
            "[1 0] 0.9891298042443863\n",
            "[1 1] 0.015552778145753128\n",
            "epochs: 6.0\n",
            "[0 0] 0.005625211435815758\n",
            "[0 1] 0.9922099656276941\n",
            "[1 0] 0.9915443580479175\n",
            "[1 1] 0.01694332658239157\n",
            "epochs: 7.0\n",
            "[0 0] 0.0019544398675402134\n",
            "[0 1] 0.9934850143000605\n",
            "[1 0] 0.9934672674082785\n",
            "[1 1] 0.0007886110283729943\n",
            "epochs: 8.0\n",
            "[0 0] 0.0036493566842660274\n",
            "[0 1] 0.9950489745378326\n",
            "[1 0] 0.9943655959618727\n",
            "[1 1] 0.003149359618503074\n",
            "epochs: 9.0\n",
            "[0 0] 0.0023304944233086483\n",
            "[0 1] 0.9958242355532402\n",
            "[1 0] 0.9953474336963716\n",
            "[1 1] 0.006106035948582399\n",
            "Final prediction\n",
            "[0 0] 0.003032173692500074\n",
            "[0 1] 0.9963860761357731\n",
            "[1 0] 0.9959034563937058\n",
            "[1 1] 0.0006386449217581\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEGCAYAAAB7DNKzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlYAKvhd+rzAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4BuxSMNOqP0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}